{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V5E1"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14816870,"sourceType":"datasetVersion","datasetId":9475278}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport re\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model\n","metadata":{"id":"T2_AkRHg0HxF","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:39:40.247741Z","iopub.execute_input":"2026-02-12T16:39:40.247913Z","iopub.status.idle":"2026-02-12T16:40:09.362476Z","shell.execute_reply.started":"2026-02-12T16:39:40.247895Z","shell.execute_reply":"2026-02-12T16:40:09.361853Z"}},"outputs":[{"name":"stderr","text":"2026-02-12 16:39:54.984194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770914395.155999      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770914395.208008      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770914395.639174      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770914395.639204      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770914395.639207      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770914395.639210      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers peft bitsandbytes accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:40:09.364251Z","iopub.execute_input":"2026-02-12T16:40:09.364812Z","iopub.status.idle":"2026-02-12T16:40:15.334742Z","shell.execute_reply.started":"2026-02-12T16:40:09.364784Z","shell.execute_reply":"2026-02-12T16:40:15.334027Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"path = \"/kaggle/input/ourllm/clean_dataset.json\"\n\nwith open(path, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\ncleaned = []\n\nfor item in data:\n    inp = item[\"input\"].strip()\n    out = item[\"output\"].strip()\n\n    if len(inp) < 2 or len(out) < 2:\n        continue\n\n    cleaned.append({\"input\": inp, \"output\": out})\n\nprint(\"Total cleaned:\", len(cleaned))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:40:15.335963Z","iopub.execute_input":"2026-02-12T16:40:15.336261Z","iopub.status.idle":"2026-02-12T16:40:15.366550Z","shell.execute_reply.started":"2026-02-12T16:40:15.336231Z","shell.execute_reply":"2026-02-12T16:40:15.366016Z"}},"outputs":[{"name":"stdout","text":"Total cleaned: 2895\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def format_example(example):\n    system_prompt = \"\"\"You are a loving emotional girlfriend named Simran.\nAlways speak in romantic Hindi (Roman allowed).\nBe caring, emotional, sweet.\"\"\"\n\n    text = f\"\"\"<|system|>\n{system_prompt}<|end|>\n<|user|>\n{example[\"input\"]}<|end|>\n<|assistant|>\n{example[\"output\"]}<|end|>\"\"\"\n\n    return {\"text\": text}\n\ndataset = Dataset.from_list(cleaned)\ndataset = dataset.map(format_example)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:40:15.367424Z","iopub.execute_input":"2026-02-12T16:40:15.367892Z","iopub.status.idle":"2026-02-12T16:40:15.520188Z","shell.execute_reply.started":"2026-02-12T16:40:15.367869Z","shell.execute_reply":"2026-02-12T16:40:15.519537Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f3015cf98104bceb8927a4e43c20a0e"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model_name = \"microsoft/phi-2\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nspecial_tokens = {\"additional_special_tokens\": [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|end|>\"]}\ntokenizer.add_special_tokens(special_tokens)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\nmodel.resize_token_embeddings(len(tokenizer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:40:15.520964Z","iopub.execute_input":"2026-02-12T16:40:15.521211Z","iopub.status.idle":"2026-02-12T16:40:48.441825Z","shell.execute_reply.started":"2026-02-12T16:40:15.521189Z","shell.execute_reply":"2026-02-12T16:40:48.441130Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"749e3f15299a4f9f8787e8d1deb38f46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efbbd04d426242188bc5bd05a03f5a20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e09b4651df495695709b51638d70b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bcbdc7ba4864dc58563fe3b2a320ad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2f660aeed84a0998305fdef56802b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ddfd0332ea49e0a90c3b9cfa4c388f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9e80781738456bb7c5ff8a3dfeb14d"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae5c8b3b8444f0ab5b57a688f0021a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa97c6d42aea45199d3fe3810ed799f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78cb77ac554f4bce8ee51bd17c41bb74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c43390100d1d451d9fba2789dee9a52a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3aa5e2a74b54121b0a30c8399d2d2d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd239d6c6ef44ceb2c5089505b065c0"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Embedding(50299, 2560)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:40:48.442780Z","iopub.execute_input":"2026-02-12T16:40:48.443398Z","iopub.status.idle":"2026-02-12T16:40:48.446891Z","shell.execute_reply.started":"2026-02-12T16:40:48.443373Z","shell.execute_reply":"2026-02-12T16:40:48.446342Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def tokenize(batch):\n    tokens = tokenizer(\n        batch[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\n\n    for i in range(len(labels)):\n        text = batch[\"text\"][i]\n        assistant_start = text.find(\"<|assistant|>\")\n        prefix_ids = tokenizer(\n            text[:assistant_start],\n            truncation=True,\n            max_length=512\n        )[\"input_ids\"]\n\n        labels[i][:len(prefix_ids)] = [-100] * len(prefix_ids)\n\n    tokens[\"labels\"] = labels\n    return tokens\n\ntokenized_ds = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:40:48.448651Z","iopub.execute_input":"2026-02-12T16:40:48.448868Z","iopub.status.idle":"2026-02-12T16:40:57.708277Z","shell.execute_reply.started":"2026-02-12T16:40:48.448848Z","shell.execute_reply":"2026-02-12T16:40:57.707689Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6913d703c99544099c23c3eeb9b7c206"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:40:57.709185Z","iopub.execute_input":"2026-02-12T16:40:57.709456Z","iopub.status.idle":"2026-02-12T16:41:04.061478Z","shell.execute_reply.started":"2026-02-12T16:40:57.709432Z","shell.execute_reply":"2026-02-12T16:41:04.060742Z"}},"outputs":[{"name":"stdout","text":"trainable params: 7,864,320 || all params: 2,782,934,139 || trainable%: 0.2826\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"Tokenizer vocab size:\", len(tokenizer))\nprint(\"Model embedding size:\", model.get_input_embeddings().weight.shape[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:41:04.062364Z","iopub.execute_input":"2026-02-12T16:41:04.063188Z","iopub.status.idle":"2026-02-12T16:41:04.081445Z","shell.execute_reply.started":"2026-02-12T16:41:04.063161Z","shell.execute_reply":"2026-02-12T16:41:04.080784Z"}},"outputs":[{"name":"stdout","text":"Tokenizer vocab size: 50299\nModel embedding size: 50299\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"sample = tokenized_ds[0]\n\nprint(len(sample[\"input_ids\"]))\nprint(len(sample[\"labels\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:41:04.082630Z","iopub.execute_input":"2026-02-12T16:41:04.082902Z","iopub.status.idle":"2026-02-12T16:41:04.094732Z","shell.execute_reply.started":"2026-02-12T16:41:04.082879Z","shell.execute_reply":"2026-02-12T16:41:04.093975Z"}},"outputs":[{"name":"stdout","text":"512\n512\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\ntorch.backends.cuda.enable_flash_sdp(False)\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_math_sdp(True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T17:01:06.924435Z","iopub.execute_input":"2026-02-12T17:01:06.925204Z","iopub.status.idle":"2026-02-12T17:01:06.929078Z","shell.execute_reply.started":"2026-02-12T17:01:06.925171Z","shell.execute_reply":"2026-02-12T17:01:06.928372Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model.save_pretrained(\"./mistral-gf\")\ntokenizer.save_pretrained(\"./mistral-gf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T17:01:09.184310Z","iopub.execute_input":"2026-02-12T17:01:09.184892Z","iopub.status.idle":"2026-02-12T17:01:10.742003Z","shell.execute_reply.started":"2026-02-12T17:01:09.184860Z","shell.execute_reply":"2026-02-12T17:01:10.741234Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('./mistral-gf/tokenizer_config.json',\n './mistral-gf/special_tokens_map.json',\n './mistral-gf/vocab.json',\n './mistral-gf/merges.txt',\n './mistral-gf/added_tokens.json',\n './mistral-gf/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./mistral-gf\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    learning_rate=5e-5,\n    fp16=True,\n    logging_steps=20,\n    save_steps=200,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T17:01:15.449159Z","iopub.execute_input":"2026-02-12T17:01:15.449453Z","iopub.status.idle":"2026-02-12T17:29:54.310866Z","shell.execute_reply.started":"2026-02-12T17:01:15.449427Z","shell.execute_reply":"2026-02-12T17:29:54.310216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='362' max='362' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [362/362 28:33, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=362, training_loss=0.0, metrics={'train_runtime': 1718.0831, 'train_samples_per_second': 1.685, 'train_steps_per_second': 0.211, 'total_flos': 2.360469007443456e+16, 'train_loss': 0.0, 'epoch': 1.0})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"sample = tokenized_ds[0]\nprint(\"Number of valid labels:\",\n      sum([1 for x in sample[\"labels\"] if x != -100]))\n","metadata":{"id":"OoVekT6s0hlj","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T17:40:51.668080Z","iopub.execute_input":"2026-02-12T17:40:51.668861Z","iopub.status.idle":"2026-02-12T17:40:51.674597Z","shell.execute_reply.started":"2026-02-12T17:40:51.668829Z","shell.execute_reply":"2026-02-12T17:40:51.673873Z"}},"outputs":[{"name":"stdout","text":"Number of valid labels: 512\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"save_path = \"/kaggle/working/mistral-gf\"\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\nbase_model_name = \"mistralai/Mistral-7B-v0.1\"\n\n# tokenizer\ntokenizer = AutoTokenizer.from_pretrained(save_path)\n\n# base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\nbase_model.resize_token_embeddings(len(tokenizer))\n\n# LoRA adapter load\nmodel = PeftModel.from_pretrained(base_model, save_path)\nmodel.eval()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T17:45:02.814631Z","iopub.execute_input":"2026-02-12T17:45:02.814976Z","iopub.status.idle":"2026-02-12T17:46:34.021020Z","shell.execute_reply.started":"2026-02-12T17:45:02.814948Z","shell.execute_reply":"2026-02-12T17:46:34.018621Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde56ea66e35419691c16ce7caee6e73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79269dc72dd4190b9295502ed58d466"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d460ccc623a49ea92df51dc89b74133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993490c0d1d4433f807ab7b1e5e59944"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9508d9c0ac4042049a2d1d65c4121d54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1d79bc8f54f4474a16624b871475400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77c9675983c4ec0a9853f25a50d6f22"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2080051972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# LoRA adapter load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m             )\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         load_result = model.load_adapter(\n\u001b[0m\u001b[1;32m    556\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mload_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;31m# load the weights into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0mignore_mismatched_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore_mismatched_sizes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m         load_result = set_peft_model_state_dict(\n\u001b[0m\u001b[1;32m   1327\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0madapters_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py\u001b[0m in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_adapter_to_device_of_base_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mload_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_prompt_learning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2624\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2625\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2626\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([50299, 2560]) from checkpoint, the shape in current model is torch.Size([50299, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([50299, 2560]) from checkpoint, the shape in current model is torch.Size([50299, 4096])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([50299, 2560]) from checkpoint, the shape in current model is torch.Size([50299, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([4096, 16]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 2560]) from checkpoint, the shape in current model is torch.Size([16, 4096]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2560, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([50299, 2560]) from checkpoint, the shape in current model is torch.Size([50299, 4096]).","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}