# -*- coding: utf-8 -*-
"""ProjcectLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uYfXh0VZj7oQH0QPYNsYgPax2Ri7HtLY
"""

import numpy as np
import pandas as pd

v = pd.read_csv("/content/hinglish_conversations.csv",
                on_bad_lines='skip',
                engine='python')

v.head(20)

v.shape

v.to_csv("cleaned_dataset.csv", index=False)

s = pd.read_csv("/content/cleaned_dataset.csv")

s.shape

s.columns

s.isnull().sum()

"""### LLM Fine-Tuning starts from here"""

# Converting Dataset into Training Form

import json

train_data = []

for i in range(len(v)):
    user = str(v.iloc[i]["input"])
    bot = str(v.iloc[i]["output"])

    text = f"User: {user}\nBot: {bot}"

    train_data.append({"text": text})

# Save to JSONL file
with open("train.jsonl", "w") as f:
    for item in train_data:
        f.write(json.dumps(item) + "\n")

print("train.jsonl")

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "Abhishekcr448/Tiny-Hinglish-Chat-21M"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

print("Model + Tokenizer Loaded!")

from datasets import load_dataset

# load JSONL
dataset = load_dataset("json", data_files="train.jsonl")

# tokenize
def tokenize(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

# remove original text column
tokenized_dataset = tokenized_dataset.remove_columns(["text"])

# ADD LABELS
tokenized_dataset = tokenized_dataset.map(
    lambda batch: {"labels": batch["input_ids"]},
    batched=True
)

print("Tokenization + Labels âœ”")
tokenized_dataset

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="hinglish-chatbot",
    per_device_train_batch_size=2,
    num_train_epochs=1,
    learning_rate=2e-5,
    logging_steps=50,
    save_steps=500,
    fp16=True,         # GPU speed boost
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)

trainer.train()

# vhfh