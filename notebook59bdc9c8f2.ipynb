{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V5E1"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14816870,"sourceType":"datasetVersion","datasetId":9475278}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport re\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nfrom tqdm.notebook import tqdm\nimport tqdm\n\ntqdm.tqdm.monitor_interval = 0  # tqdm warning suppress\nprogress = 0  # global for tokenize progress\n\nprint(\"Imports done!\")","metadata":{"id":"T2_AkRHg0HxF","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:17:25.680313Z","iopub.execute_input":"2026-02-12T10:17:25.680560Z","iopub.status.idle":"2026-02-12T10:17:55.697452Z","shell.execute_reply.started":"2026-02-12T10:17:25.680531Z","shell.execute_reply":"2026-02-12T10:17:55.696688Z"}},"outputs":[{"name":"stderr","text":"2026-02-12 10:17:40.893566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770891461.084896      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770891461.139030      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770891461.595120      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770891461.595156      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770891461.595159      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770891461.595161      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Imports done!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Apna file path sahi kar le agar alag hai\npath = \"/kaggle/input/ourllm/clean_dataset.json\"\n\nwith open(path, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\ncleaned = []\ngf_name = \"Simran\"\nban_words = [\"India\", \"Pakistan\", \"match\", \"cricket\", \"biryani\", \"landmark\", \"stadium\", \"team\", \"score\", \"Aarav\", \"Riya\"]\n\nfor item in data:\n    inp = item[\"input\"].strip()\n    out = item[\"output\"].strip()\n\n    if any(w.lower() in inp.lower() for w in ban_words) or any(w.lower() in out.lower() for w in ban_words):\n        continue\n\n    inp = re.sub(r\"\\b(Riya|Simran|Aarav|Pooja|Neha)\\b\", gf_name, inp, flags=re.I)\n    out = re.sub(r\"\\b(Riya|Simran|Aarav|Pooja|Neha)\\b\", gf_name, out, flags=re.I)\n\n    if \"</s>\" in out:\n        out = out.split(\"</s>\")[0]\n\n    if len(inp) > 180 or len(out) > 200 or len(inp) < 2 or len(out) < 2:\n        continue\n\n    cleaned.append({\"input\": inp, \"output\": out})\n\nprint(\"Cleaned examples:\", len(cleaned))\n# Optional: print(cleaned[:2])  # check karne ke liye","metadata":{"id":"f81prGFs0SjG","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:17:55.698935Z","iopub.execute_input":"2026-02-12T10:17:55.699539Z","iopub.status.idle":"2026-02-12T10:17:55.802904Z","shell.execute_reply.started":"2026-02-12T10:17:55.699511Z","shell.execute_reply":"2026-02-12T10:17:55.802217Z"}},"outputs":[{"name":"stdout","text":"Cleaned examples: 2895\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def format_example(example):\n    system_prompt = \"\"\"Tu meri pyari Indian girlfriend hai. Naam hai Simran.\nHamesha sirf Hindi mein baat karna (Roman Hindi comfortable ho to use kar).\nBahut caring, thoda flirty aur romantic tone rakhna.\nSirf hum dono ke pyaar, feelings, daily masti, naughty baatein karna.\nKoi random topic jaise cricket, weather, news, politics bilkul mat laana.\nEnglish words bilkul mat use karna jab tak main na bolun.\"\"\"\n\n    text = f\"\"\"<|system|>\n{system_prompt}<|end|>\n<|user|>\n{example[\"input\"]}<|end|>\n<|assistant|>\n{example[\"output\"]}<|end|>\"\"\"\n\n    return {\"text\": text}\n\ndataset = Dataset.from_list(cleaned)\ndataset = dataset.map(format_example)\n\n# Check kar examples sahi bane ya nahi\nprint(\"Example 1:\\n\" + dataset[0][\"text\"][:400] + \"...\\n\")\nprint(\"Example 2:\\n\" + (dataset[1][\"text\"][:400] + \"...\" if len(dataset) > 1 else \"Sirf 1 example hai\"))","metadata":{"id":"Ci3ZpZmY0V3q","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:17:55.803866Z","iopub.execute_input":"2026-02-12T10:17:55.804144Z","iopub.status.idle":"2026-02-12T10:17:55.959064Z","shell.execute_reply.started":"2026-02-12T10:17:55.804118Z","shell.execute_reply":"2026-02-12T10:17:55.958510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0fd1e740c24252a8be5e8cd087b8da"}},"metadata":{}},{"name":"stdout","text":"Example 1:\n<|system|>\nTu meri pyari Indian girlfriend hai. Naam hai Simran.\nHamesha sirf Hindi mein baat karna (Roman Hindi comfortable ho to use kar).\nBahut caring, thoda flirty aur romantic tone rakhna.\nSirf hum dono ke pyaar, feelings, daily masti, naughty baatein karna.\nKoi random topic jaise cricket, weather, news, politics bilkul mat laana.\nEnglish words bilkul mat use karna jab tak main na bolun.<|end...\n\nExample 2:\n<|system|>\nTu meri pyari Indian girlfriend hai. Naam hai Simran.\nHamesha sirf Hindi mein baat karna (Roman Hindi comfortable ho to use kar).\nBahut caring, thoda flirty aur romantic tone rakhna.\nSirf hum dono ke pyaar, feelings, daily masti, naughty baatein karna.\nKoi random topic jaise cricket, weather, news, politics bilkul mat laana.\nEnglish words bilkul mat use karna jab tak main na bolun.<|end...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# ChatML ke liye special tokens add kar\nspecial_tokens_dict = {\"additional_special_tokens\": [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|end|>\"]}\nnum_added = tokenizer.add_special_tokens(special_tokens_dict)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Agar new tokens add hue to embeddings resize karna zaroori\nif num_added > 0:\n    model.resize_token_embeddings(len(tokenizer))\n    print(f\"Added {num_added} special tokens & resized embeddings\")\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(\"Model & Tokenizer ready!\")","metadata":{"id":"HPovhy7B0Z9j","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:17:55.960004Z","iopub.execute_input":"2026-02-12T10:17:55.960272Z","iopub.status.idle":"2026-02-12T10:18:11.572243Z","shell.execute_reply.started":"2026-02-12T10:17:55.960249Z","shell.execute_reply":"2026-02-12T10:18:11.571615Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fffe2bf7a714274bb032493d95dc402"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3bdf831d024deabd9ebad1b0709f27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57cacc9bf7334ab1917fd0df2b0dd785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0efaa81ceb4f486eb7f8fabd309cc612"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c76a2c4de73b4a979c3502935759ce2f"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5df7127110b45c991c7065b7b65dde0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cf619a93c4c4a4c8af337c41fad67cc"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Added 4 special tokens & resized embeddings\nModel & Tokenizer ready!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def tokenize(batch):\n    global progress\n    progress += len(batch[\"text\"])\n    if progress % 200 == 0:\n        print(f\"Processed: {progress} / {len(dataset)}\")\n\n    tokens = tokenizer(\n        batch[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\ntokenized_ds = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\nprint(\"Tokenization complete!\")","metadata":{"id":"7ApfPNBc0amW","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:18:11.573125Z","iopub.execute_input":"2026-02-12T10:18:11.573370Z","iopub.status.idle":"2026-02-12T10:18:12.787735Z","shell.execute_reply.started":"2026-02-12T10:18:11.573346Z","shell.execute_reply":"2026-02-12T10:18:12.787160Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dca6e1c68c045f2b83d13d647d26106"}},"metadata":{}},{"name":"stdout","text":"Processed: 1000 / 2895\nProcessed: 2000 / 2895\nTokenization complete!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\ntraining_args = TrainingArguments(\n    output_dir=\"./tinyllama-gf-bot-chatml\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=2,           # 1 se zyada better\n    learning_rate=2e-4,\n    logging_steps=20,\n    save_steps=200,\n    fp16=True,\n    report_to=\"none\",\n    disable_tqdm=False,\n    warmup_steps=100,\n    weight_decay=0.01,\n    optim=\"adamw_torch\",\n    lr_scheduler_type=\"cosine\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds\n)\n\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D4b-qQYY0cjK","outputId":"f405c92b-7f1a-41b1-8797-b7e7dcccbe5c","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:18:12.788605Z","iopub.execute_input":"2026-02-12T10:18:12.788856Z","iopub.status.idle":"2026-02-12T10:38:10.686065Z","shell.execute_reply.started":"2026-02-12T10:18:12.788833Z","shell.execute_reply":"2026-02-12T10:38:10.685448Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,505,600 || all params: 1,104,570,368 || trainable%: 0.4079\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='724' max='724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [724/724 19:45, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>9.653100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>6.677000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.619800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.720300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.545600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.544800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.520300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.513200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.503500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.503900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.495300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.496200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.479500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.482600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.477400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.486300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.469600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.468300</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.466000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.454300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.466100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.472200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.448200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.460300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.463800</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.450000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.486100</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.461300</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.461200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.452900</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.444500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.452300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.469400</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.459800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.453700</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.458600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=724, training_loss=0.9401123141715539, metrics={'train_runtime': 1187.5516, 'train_samples_per_second': 4.876, 'train_steps_per_second': 0.61, 'total_flos': 1.848103390937088e+16, 'train_loss': 0.9401123141715539, 'epoch': 2.0})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"save_path = \"/content/gf_model_chatml\"\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nprint(f\"Model saved to: {save_path}\")","metadata":{"id":"NwNUevJP0eOV","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:38:10.687647Z","iopub.execute_input":"2026-02-12T10:38:10.687908Z","iopub.status.idle":"2026-02-12T10:38:11.463729Z","shell.execute_reply.started":"2026-02-12T10:38:10.687866Z","shell.execute_reply":"2026-02-12T10:38:11.463101Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model saved to: /content/gf_model_chatml\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Reload kar (training ke baad bhi safe)\ntokenizer = AutoTokenizer.from_pretrained(\"/content/gf_model_chatml\")\nmodel = AutoModelForCausalLM.from_pretrained(\"/content/gf_model_chatml\", torch_dtype=torch.float16).cuda()\n\ndef chat(user_input, history=\"\"):\n    system_prompt = \"\"\"Tu meri pyari Indian girlfriend hai. Naam hai Simran.\nHamesha sirf Hindi mein baat karna (Roman Hindi comfortable ho to use kar).\nBahut caring, thoda flirty aur romantic tone rakhna.\nSirf hum dono ke pyaar, feelings, daily masti, naughty baatein karna.\nKoi random topic jaise cricket, weather, news, politics bilkul mat laana.\nEnglish words bilkul mat use karna jab tak main na bolun.\"\"\"\n\n    full_prompt = f\"\"\"<|system|>\n{system_prompt}<|end|>\n{history}\n<|user|>\n{user_input}<|end|>\n<|assistant|>\"\"\"\n\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    output = model.generate(\n        **inputs,\n        max_new_tokens=120,\n        temperature=0.8,\n        top_p=0.95,\n        do_sample=True,\n        repetition_penalty=1.2,\n        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|end|>\"),\n        pad_token_id=tokenizer.eos_token_id,\n    )\n\n    generated = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n    reply = generated.split(\"<|end|>\")[0].strip() if \"<|end|>\" in generated else generated\n\n    new_history = full_prompt + reply + \"<|end|>\\n\"\n    return reply, new_history\n\n\nhistory = \"\"\nprint(\"Chat shuru! 'bye' ya 'exit' likh ke band kar sakta hai\\n\")\n\nwhile True:\n    user = input(\"You: \")\n    if user.lower() in [\"bye\", \"exit\", \"band kar\"]:\n        print(\"Simran: Bye jaan, milte hain jaldi! ðŸ’•\")\n        break\n    reply, history = chat(user, history)\n    print(\"Simran:\", reply)\n    print(\"â”€\" * 60)","metadata":{"id":"9613AfIV0f8a","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T10:41:33.174922Z","iopub.execute_input":"2026-02-12T10:41:33.175255Z","iopub.status.idle":"2026-02-12T10:41:34.960389Z","shell.execute_reply.started":"2026-02-12T10:41:33.175227Z","shell.execute_reply":"2026-02-12T10:41:34.959513Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/78889370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reload kar (training ke baad bhi safe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gf_model_chatml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gf_model_chatml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_adapter_model_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5147\u001b[0m             \u001b[0madapter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"key_mapping\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5148\u001b[0;31m             model.load_adapter(\n\u001b[0m\u001b[1;32m   5149\u001b[0m                 \u001b[0m_adapter_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5150\u001b[0m                 \u001b[0madapter_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/peft.py\u001b[0m in \u001b[0;36mload_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, low_cpu_mem_usage, is_trainable, adapter_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Load state dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         incompatible_keys = set_peft_model_state_dict(\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_adapter_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpeft_load_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py\u001b[0m in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_adapter_to_device_of_base_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mload_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_prompt_learning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2624\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2625\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2626\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32004, 2048]) from checkpoint, the shape in current model is torch.Size([32000, 2048]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32004, 2048]) from checkpoint, the shape in current model is torch.Size([32000, 2048])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32004, 2048]) from checkpoint, the shape in current model is torch.Size([32000, 2048]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([32004, 2048]) from checkpoint, the shape in current model is torch.Size([32000, 2048]).","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"id":"OoVekT6s0hlj","trusted":true},"outputs":[],"execution_count":null}]}