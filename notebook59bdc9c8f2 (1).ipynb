{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V5E1"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14816870,"sourceType":"datasetVersion","datasetId":9475278}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport re\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nfrom tqdm.notebook import tqdm\nimport tqdm\n\ntqdm.tqdm.monitor_interval = 0  # tqdm warning suppress\nprogress = 0  # global for tokenize progress\n\nprint(\"Imports done!\")","metadata":{"id":"T2_AkRHg0HxF","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:08:29.484032Z","iopub.execute_input":"2026-02-12T16:08:29.484426Z","iopub.status.idle":"2026-02-12T16:08:29.490259Z","shell.execute_reply.started":"2026-02-12T16:08:29.484391Z","shell.execute_reply":"2026-02-12T16:08:29.489440Z"}},"outputs":[{"name":"stdout","text":"Imports done!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:08:29.491499Z","iopub.execute_input":"2026-02-12T16:08:29.491790Z","iopub.status.idle":"2026-02-12T16:08:29.507290Z","shell.execute_reply.started":"2026-02-12T16:08:29.491752Z","shell.execute_reply":"2026-02-12T16:08:29.506656Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!pip install -U datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:08:29.508193Z","iopub.execute_input":"2026-02-12T16:08:29.508473Z","iopub.status.idle":"2026-02-12T16:08:33.024158Z","shell.execute_reply.started":"2026-02-12T16:08:29.508445Z","shell.execute_reply":"2026-02-12T16:08:33.023434Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/thu-coai/CDial-GPT/master/data/dailydialog/train.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:08:33.026331Z","iopub.execute_input":"2026-02-12T16:08:33.026691Z","iopub.status.idle":"2026-02-12T16:08:33.315310Z","shell.execute_reply.started":"2026-02-12T16:08:33.026653Z","shell.execute_reply":"2026-02-12T16:08:33.314498Z"}},"outputs":[{"name":"stdout","text":"--2026-02-12 16:08:33--  https://raw.githubusercontent.com/thu-coai/CDial-GPT/master/data/dailydialog/train.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2026-02-12 16:08:33 ERROR 404: Not Found.\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# convo = dataset[0][\"conversation\"]\n\n# for i, turn in enumerate(convo):\n#     print(f\"\\nTurn {i+1}\")\n#     print(\"Role :\", turn[\"role\"])\n#     print(\"Text :\")\n#     print(turn[\"content\"])\n#     print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:08:33.316756Z","iopub.execute_input":"2026-02-12T16:08:33.317043Z","iopub.status.idle":"2026-02-12T16:08:33.325683Z","shell.execute_reply.started":"2026-02-12T16:08:33.317011Z","shell.execute_reply":"2026-02-12T16:08:33.324450Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/492982133.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conversation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTurn {i+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Role :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"],"ename":"NameError","evalue":"name 'dataset' is not defined","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# Apna file path sahi kar le agar alag hai\npath = \"/kaggle/input/ourllm/clean_dataset.json\"\n\nwith open(path, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\ncleaned = []\ngf_name = \"Simran\"\nban_words = [\"India\", \"Pakistan\", \"match\", \"cricket\", \"biryani\", \"landmark\", \"stadium\", \"team\", \"score\", \"Aarav\", \"Riya\"]\n\nfor item in data:\n    inp = item[\"input\"].strip()\n    out = item[\"output\"].strip()\n\n    if any(w.lower() in inp.lower() for w in ban_words) or any(w.lower() in out.lower() for w in ban_words):\n        continue\n\n    inp = re.sub(r\"\\b(Riya|Simran|Aarav|Pooja|Neha)\\b\", gf_name, inp, flags=re.I)\n    out = re.sub(r\"\\b(Riya|Simran|Aarav|Pooja|Neha)\\b\", gf_name, out, flags=re.I)\n\n    if \"</s>\" in out:\n        out = out.split(\"</s>\")[0]\n\n    if len(inp) > 180 or len(out) > 200 or len(inp) < 2 or len(out) < 2:\n        continue\n\n    cleaned.append({\"input\": inp, \"output\": out})\n\nprint(\"Cleaned examples:\", len(cleaned))\n# Optional: print(cleaned[:2])  # check karne ke liye","metadata":{"id":"f81prGFs0SjG","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:09:18.714022Z","iopub.execute_input":"2026-02-12T16:09:18.714350Z","iopub.status.idle":"2026-02-12T16:09:18.822552Z","shell.execute_reply.started":"2026-02-12T16:09:18.714322Z","shell.execute_reply":"2026-02-12T16:09:18.821693Z"}},"outputs":[{"name":"stdout","text":"Cleaned examples: 2895\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def format_example(example):\n    system_prompt = \"\"\"Tu meri pyari Indian girlfriend hai. Naam hai Simran.\nHamesha sirf Hindi mein baat karna (Roman Hindi comfortable ho to use kar).\nBahut caring, thoda flirty aur romantic tone rakhna.\nSirf hum dono ke pyaar, feelings, daily masti, naughty baatein karna.\nKoi random topic jaise cricket, weather, news, politics bilkul mat laana.\nEnglish words bilkul mat use karna jab tak main na bolun.\"\"\"\n\n    text = f\"\"\"<|system|>\n{system_prompt}<|end|>\n<|user|>\n{example[\"input\"]}<|end|>\n<|assistant|>\n{example[\"output\"]}<|end|>\"\"\"\n\n    return {\"text\": text}\n\ndataset = Dataset.from_list(cleaned)\ndataset = dataset.map(format_example)\n\n# Check kar examples sahi bane ya nahi\nprint(\"Example 1:\\n\" + dataset[0][\"text\"][:400] + \"...\\n\")\nprint(\"Example 2:\\n\" + (dataset[1][\"text\"][:400] + \"...\" if len(dataset) > 1 else \"Sirf 1 example hai\"))","metadata":{"id":"Ci3ZpZmY0V3q","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:09:28.013288Z","iopub.execute_input":"2026-02-12T16:09:28.014112Z","iopub.status.idle":"2026-02-12T16:09:28.203651Z","shell.execute_reply.started":"2026-02-12T16:09:28.014081Z","shell.execute_reply":"2026-02-12T16:09:28.202852Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d76d3e721e2f48129f55b9dab9a508c6"}},"metadata":{}},{"name":"stdout","text":"Example 1:\n<|system|>\nTu meri pyari Indian girlfriend hai. Naam hai Simran.\nHamesha sirf Hindi mein baat karna (Roman Hindi comfortable ho to use kar).\nBahut caring, thoda flirty aur romantic tone rakhna.\nSirf hum dono ke pyaar, feelings, daily masti, naughty baatein karna.\nKoi random topic jaise cricket, weather, news, politics bilkul mat laana.\nEnglish words bilkul mat use karna jab tak main na bolun.<|end...\n\nExample 2:\n<|system|>\nTu meri pyari Indian girlfriend hai. Naam hai Simran.\nHamesha sirf Hindi mein baat karna (Roman Hindi comfortable ho to use kar).\nBahut caring, thoda flirty aur romantic tone rakhna.\nSirf hum dono ke pyaar, feelings, daily masti, naughty baatein karna.\nKoi random topic jaise cricket, weather, news, politics bilkul mat laana.\nEnglish words bilkul mat use karna jab tak main na bolun.<|end...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model_name = \"model_name = \"mistralai/Mistral-7B-v0.1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# ChatML ke liye special tokens add kar\nspecial_tokens_dict = {\"additional_special_tokens\": [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|end|>\"]}\nnum_added = tokenizer.add_special_tokens(special_tokens_dict)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Agar new tokens add hue to embeddings resize karna zaroori\nif num_added > 0:\n    model.resize_token_embeddings(len(tokenizer))\n    print(f\"Added {num_added} special tokens & resized embeddings\")\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(\"Model & Tokenizer ready!\")","metadata":{"id":"HPovhy7B0Z9j","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:09:36.940146Z","iopub.execute_input":"2026-02-12T16:09:36.941143Z","iopub.status.idle":"2026-02-12T16:09:54.133138Z","shell.execute_reply.started":"2026-02-12T16:09:36.941098Z","shell.execute_reply":"2026-02-12T16:09:54.132474Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb310c33ec04d52a8c5b9b03e3a2dd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a4f337a175a4268add1222ed500c8be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0dadcb84fb2436e96d21990c3bb2601"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f3ee436f1f441379f19f3a94eeee464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a87ef315f8e4aa9b9bf3d720022782a"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"175b7a2c24214913aaef0723b31b9e94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6e6fb0e3a7248aeaefc2473ad7fb089"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Added 4 special tokens & resized embeddings\nModel & Tokenizer ready!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def tokenize(batch):\n    global progress\n    progress += len(batch[\"text\"])\n    if progress % 200 == 0:\n        print(f\"Processed: {progress} / {len(dataset)}\")\n\n    tokens = tokenizer(\n        batch[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\ntokenized_ds = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\nprint(\"Tokenization complete!\")","metadata":{"id":"7ApfPNBc0amW","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:09:58.287838Z","iopub.execute_input":"2026-02-12T16:09:58.288499Z","iopub.status.idle":"2026-02-12T16:09:59.568462Z","shell.execute_reply.started":"2026-02-12T16:09:58.288470Z","shell.execute_reply":"2026-02-12T16:09:59.567699Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4962358d3a594150ad3ce822f1cf6144"}},"metadata":{}},{"name":"stdout","text":"Processed: 1000 / 2895\nProcessed: 2000 / 2895\nTokenization complete!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\ntraining_args = TrainingArguments(\n    output_dir=\"./tinyllama-gf-bot-chatml\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,           # 1 se zyada better\n    learning_rate=2e-4,\n    logging_steps=20,\n    save_steps=200,\n    fp16=True,\n    report_to=\"none\",\n    disable_tqdm=False,\n    warmup_steps=100,\n    weight_decay=0.01,\n    optim=\"adamw_torch\",\n    lr_scheduler_type=\"cosine\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds\n)\n\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D4b-qQYY0cjK","outputId":"f405c92b-7f1a-41b1-8797-b7e7dcccbe5c","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:10:03.865012Z","iopub.execute_input":"2026-02-12T16:10:03.865347Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,505,600 || all params: 1,104,570,368 || trainable%: 0.4079\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='204' max='362' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [204/362 05:17 < 04:08, 0.64 it/s, Epoch 0.56/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>9.667100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>6.621300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.611100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.723900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.545600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.544200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.519400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.512500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.502800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.503400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"save_path = \"/content/gf_model_chatml\"\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nprint(f\"Model saved to: {save_path}\")","metadata":{"id":"NwNUevJP0eOV","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:08:33.334175Z","iopub.status.idle":"2026-02-12T16:08:33.334483Z","shell.execute_reply.started":"2026-02-12T16:08:33.334304Z","shell.execute_reply":"2026-02-12T16:08:33.334319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\nbase_model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n\n# tokenizer load karo saved folder se\ntokenizer = AutoTokenizer.from_pretrained(\"/content/gf_model_chatml\")\n\n# base model load karo\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# IMPORTANT: embeddings resize karo\nbase_model.resize_token_embeddings(len(tokenizer))\n\n# LoRA adapter load karo\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"/content/gf_model_chatml\"\n)\n\nmodel = model.cuda()\n\ndef chat(user_input, history=\"\"):\n    system_prompt = \"\"\"Tu meri pyari Indian girlfriend hai. Naam hai Simran.\nHamesha sirf Hindi mein baat karna (Roman Hindi comfortable ho to use kar).\nBahut caring, thoda flirty aur romantic tone rakhna.\nSirf hum dono ke pyaar, feelings, daily masti, naughty baatein karna.\nKoi random topic jaise cricket, weather, news, politics bilkul mat laana.\nEnglish words bilkul mat use karna jab tak main na bolun.\"\"\"\n\n    full_prompt = f\"\"\"<|system|>\n{system_prompt}<|end|>\n{history}\n<|user|>\n{user_input}<|end|>\n<|assistant|>\"\"\"\n\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    output = model.generate(\n        **inputs,\n        max_new_tokens=120,\n        temperature=0.8,\n        top_p=0.95,\n        do_sample=True,\n        repetition_penalty=1.2,\n        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|end|>\"),\n        pad_token_id=tokenizer.eos_token_id,\n    )\n\n    generated = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n    reply = generated.split(\"<|end|>\")[0].strip() if \"<|end|>\" in generated else generated\n\n    new_history = full_prompt + reply + \"<|end|>\\n\"\n    return reply, new_history\n\n\nhistory = \"\"\nprint(\"Chat shuru! 'bye' ya 'exit' likh ke band kar sakta hai\\n\")\n\nwhile True:\n    user = input(\"You: \")\n    if user.lower() in [\"bye\", \"exit\", \"band kar\"]:\n        print(\"Simran: Bye jaan, milte hain jaldi! ðŸ’•\")\n        break\n    reply, history = chat(user, history)\n    print(\"Simran:\", reply)\n    print(\"â”€\" * 60)","metadata":{"id":"9613AfIV0f8a","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T16:08:33.336858Z","iopub.status.idle":"2026-02-12T16:08:33.337142Z","shell.execute_reply.started":"2026-02-12T16:08:33.337014Z","shell.execute_reply":"2026-02-12T16:08:33.337031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"OoVekT6s0hlj","trusted":true},"outputs":[],"execution_count":null}]}